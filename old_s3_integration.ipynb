{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "setup-title",
      "metadata": {},
      "source": [
        "# MedGemma Chest X-Ray Processing with AWS S3\n",
        "## Big Data Pipeline for Medical Image Analysis\n",
        "\n",
        "**Technologies:** Apache Spark, AWS S3, MedGemma, PySpark, Pandas UDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-section",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "config-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS S3 Configuration\n",
        "AWS_ACCESS_KEY = \"AKIAXQIP742MDATJ4VF5\"  # Or use IAM roles\n",
        "AWS_SECRET_KEY = \"8oRtdSmJ4/DzmLuUu4alYgvQjtTL/TDt3oDWg6kq\"  # Or use IAM roles\n",
        "AWS_REGION = \"us-east-1\"  # Change to your region\n",
        "\n",
        "# S3 Bucket Configuration\n",
        "RAW_IMAGES_BUCKET = \"s3a://medgemma-images\"\n",
        "METADATA_BUCKET = \"s3a://medgemma-metadata2\"\n",
        "RESULTS_BUCKET = \"s3a://medgemma-results\"\n",
        "\n",
        "# Image paths in S3\n",
        "IMAGES_PATH = f\"{RAW_IMAGES_BUCKET}/\"\n",
        "REPORTS_CSV = f\"{METADATA_BUCKET}/indiana_reports.csv\"\n",
        "PROJECTIONS_CSV = f\"{METADATA_BUCKET}/indiana_projections.csv\"\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_ID = \"google/medgemma-2b\"\n",
        "HF_TOKEN = \"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\"  # Get from https://huggingface.co/settings/tokens\n",
        "\n",
        "# Processing Configuration\n",
        "BATCH_SIZE = 2  # Images per partition\n",
        "MAX_NEW_TOKENS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spark-init",
      "metadata": {},
      "source": [
        "## 1. Initialize Spark Session with S3 Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0a822f34",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813860 sha256=be08d9e4c3f7e32efa77c28ea098f473de6e28f841b7b84699edd8e60fb28699\n",
            "  Stored in directory: /Users/yashavikasingh/Library/Caches/pip/wheels/31/9f/68/f89fb34ccd886909be7d0e390eaaf97f21efdf540c0ee8dbcd\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
            "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "spark-setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAVA_HOME: /opt/homebrew/opt/openjdk@17\n",
            "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
            "Available CPU cores: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/bin/spark-class: line 71: /opt/homebrew/opt/openjdk@17/bin/java: No such file or directory\n",
            "/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
            "head: illegal line count -- -1\n"
          ]
        },
        {
          "ename": "PySparkRuntimeError",
          "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable CPU cores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_cores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create Spark Session with S3 configuration\u001b[39;00m\n\u001b[32m     21\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMedGemma-S3-Pipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_cores\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m8g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m8g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.maxResultSize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.execution.arrow.pyspark.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.packages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.hadoop:hadoop-aws:3.3.4,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcom.amazonaws:aws-java-sdk-bundle:1.12.262\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.access.key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAWS_ACCESS_KEY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.secret.key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAWS_SECRET_KEY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.endpoint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms3.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_REGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.amazonaws.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.impl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.path.style.access\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.connection.ssl.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.fast.upload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.multipart.size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m104857600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.s3a.committer.name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmagic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Set log level to reduce noise\u001b[39;00m\n\u001b[32m     43\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/core/context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/core/context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/1011-hw3/lib/python3.12/site-packages/pyspark/java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
            "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pyspark.sql import SparkSession\n",
        "import multiprocessing\n",
        "\n",
        "# Set JAVA_HOME (adjust path for your system)\n",
        "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17\"  # macOS\n",
        "# For Linux: os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk\"\n",
        "os.environ[\"PATH\"] = f\"{os.environ['JAVA_HOME']}/bin:{os.environ['PATH']}\"\n",
        "\n",
        "# Verify Java installation\n",
        "print(\"JAVA_HOME:\", os.environ[\"JAVA_HOME\"])\n",
        "java_version = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
        "print(java_version.stderr.splitlines()[0])\n",
        "\n",
        "# Get available CPU cores\n",
        "n_cores = multiprocessing.cpu_count()\n",
        "print(f\"Available CPU cores: {n_cores}\")\n",
        "\n",
        "# Create Spark Session with S3 configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MedGemma-S3-Pipeline\") \\\n",
        "    .master(f\"local[{n_cores}]\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.jars.packages\", \n",
        "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
        "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{AWS_REGION}.amazonaws.com\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"✅ Spark Session Created: {spark.version}\")\n",
        "print(f\"✅ S3 Configuration: Enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify-s3",
      "metadata": {},
      "source": [
        "## 2. Verify S3 Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3-verify",
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "def check_s3_bucket(bucket_name):\n",
        "    \"\"\"Check if S3 bucket exists and is accessible\"\"\"\n",
        "    try:\n",
        "        s3_client.head_bucket(Bucket=bucket_name)\n",
        "        print(f\"✅ Bucket '{bucket_name}' is accessible\")\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response['Error']['Code']\n",
        "        if error_code == '404':\n",
        "            print(f\"❌ Bucket '{bucket_name}' does not exist\")\n",
        "        elif error_code == '403':\n",
        "            print(f\"❌ Access denied to bucket '{bucket_name}'\")\n",
        "        else:\n",
        "            print(f\"❌ Error accessing bucket '{bucket_name}': {e}\")\n",
        "        return False\n",
        "\n",
        "# Check all buckets\n",
        "print(\"Checking S3 Buckets...\")\n",
        "check_s3_bucket(RAW_IMAGES_BUCKET.replace('s3a://', ''))\n",
        "check_s3_bucket(METADATA_BUCKET.replace('s3a://', ''))\n",
        "check_s3_bucket(RESULTS_BUCKET.replace('s3a://', ''))\n",
        "\n",
        "# List files in images bucket (sample)\n",
        "try:\n",
        "    response = s3_client.list_objects_v2(\n",
        "        Bucket=RAW_IMAGES_BUCKET.replace('s3a://', ''),\n",
        "        MaxKeys=10\n",
        "    )\n",
        "    \n",
        "    if 'Contents' in response:\n",
        "        print(f\"\\n✅ Found {response.get('KeyCount', 0)} sample files in images bucket:\")\n",
        "        for obj in response['Contents'][:5]:\n",
        "            print(f\"   - {obj['Key']}\")\n",
        "    else:\n",
        "        print(\"⚠️  No files found in images bucket\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error listing files: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-data",
      "metadata": {},
      "source": [
        "## 3. Load Image Data from S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-images",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import input_file_name, split, size, element_at\n",
        "\n",
        "print(\"Loading images from S3...\")\n",
        "\n",
        "# Load all PNG images from S3\n",
        "images_df = spark.read.format(\"binaryFile\") \\\n",
        "    .option(\"pathGlobFilter\", \"*.png\") \\\n",
        "    .option(\"recursiveFileLookup\", \"true\") \\\n",
        "    .load(IMAGES_PATH) \\\n",
        "    .selectExpr(\n",
        "        \"split(path, '/')[size(split(path, '/')) - 1] as image_id\",\n",
        "        \"path as file_path\",\n",
        "        \"length as file_size_bytes\"\n",
        "    )\n",
        "\n",
        "# Cache for better performance\n",
        "images_df.cache()\n",
        "\n",
        "# Get statistics\n",
        "total_images = images_df.count()\n",
        "print(f\"\\n✅ Total images loaded: {total_images:,}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "images_df.show(5, truncate=False)\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nSchema:\")\n",
        "images_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-metadata",
      "metadata": {},
      "source": [
        "## 4. Load Metadata from S3 (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-csv",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Load reports CSV\n",
        "    reports_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(REPORTS_CSV)\n",
        "    \n",
        "    print(f\"✅ Reports CSV loaded: {reports_df.count():,} rows\")\n",
        "    print(\"\\nReports sample:\")\n",
        "    reports_df.show(5, truncate=50)\n",
        "    \n",
        "    # Load projections CSV\n",
        "    projections_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(PROJECTIONS_CSV)\n",
        "    \n",
        "    print(f\"\\n✅ Projections CSV loaded: {projections_df.count():,} rows\")\n",
        "    print(\"\\nProjections sample:\")\n",
        "    projections_df.show(5, truncate=50)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Could not load metadata CSVs: {e}\")\n",
        "    print(\"Continuing without metadata...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "demo-subset",
      "metadata": {},
      "source": [
        "## 5. Create Demo Subset for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small demo dataset for testing\n",
        "demo_df = images_df.limit(2)\n",
        "\n",
        "print(\"Demo dataset (2 images):\")\n",
        "demo_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inference-setup",
      "metadata": {},
      "source": [
        "## 6. Define Inference Function with S3 Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "define-inference",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Define output schema\n",
        "output_schema = StructType([\n",
        "    StructField(\"image_id\", StringType(), True),\n",
        "    StructField(\"file_path\", StringType(), True),\n",
        "    StructField(\"prediction\", StringType(), True),\n",
        "    StructField(\"raw_text\", StringType(), True),\n",
        "    StructField(\"confidence\", StringType(), True),\n",
        "])\n",
        "\n",
        "def predict_partition_s3(iterator):\n",
        "    \"\"\"\n",
        "    Process a partition of images from S3 using MedGemma.\n",
        "    This function runs on each Spark executor.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    from PIL import Image\n",
        "    from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "    import boto3\n",
        "    from io import BytesIO\n",
        "    import re\n",
        "    \n",
        "    # Configuration (accessible from outer scope)\n",
        "    MODEL_ID = \"google/medgemma-2b\"\n",
        "    HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN\"  # Replace with your token\n",
        "    AWS_ACCESS_KEY = \"YOUR_AWS_ACCESS_KEY\"\n",
        "    AWS_SECRET_KEY = \"YOUR_AWS_SECRET_KEY\"\n",
        "    AWS_REGION = \"us-east-1\"\n",
        "    \n",
        "    # Initialize S3 client (once per partition)\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=AWS_ACCESS_KEY,\n",
        "        aws_secret_access_key=AWS_SECRET_KEY,\n",
        "        region_name=AWS_REGION\n",
        "    )\n",
        "    \n",
        "    # Determine device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load model and processor (once per partition)\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
        "            device_map=\"auto\" if device != \"cpu\" else None,\n",
        "            token=HF_TOKEN,\n",
        "        )\n",
        "        processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "        print(\"✅ Model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Model loading error: {e}\")\n",
        "        # Return error for all batches\n",
        "        for batch in iterator:\n",
        "            yield pd.DataFrame({\n",
        "                \"image_id\": batch[\"image_id\"].tolist(),\n",
        "                \"file_path\": batch[\"file_path\"].tolist(),\n",
        "                \"prediction\": [\"ERROR\"] * len(batch),\n",
        "                \"raw_text\": [f\"Load error: {e}\"] * len(batch),\n",
        "                \"confidence\": [\"N/A\"] * len(batch),\n",
        "            })\n",
        "        return\n",
        "    \n",
        "    # Process each batch\n",
        "    for batch in iterator:\n",
        "        image_ids = batch[\"image_id\"].tolist()\n",
        "        s3_paths = batch[\"file_path\"].tolist()\n",
        "        \n",
        "        preds, texts, confidences = [], [], []\n",
        "        \n",
        "        for img_id, s3_path in zip(image_ids, s3_paths):\n",
        "            try:\n",
        "                # Parse S3 path: s3a://bucket-name/key/path\n",
        "                path_parts = s3_path.replace('s3a://', '').split('/', 1)\n",
        "                bucket_name = path_parts[0]\n",
        "                object_key = path_parts[1] if len(path_parts) > 1 else ''\n",
        "                \n",
        "                # Download image from S3\n",
        "                print(f\"Downloading: {img_id} from {bucket_name}/{object_key}\")\n",
        "                response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
        "                img_bytes = response['Body'].read()\n",
        "                img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "                \n",
        "                # Prepare messages for MedGemma\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": [{\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": (\n",
        "                                \"You are a chest X-ray triage assistant. \"\n",
        "                                \"Answer ONLY with one word: NORMAL or PNEUMONIA.\"\n",
        "                            ),\n",
        "                        }],\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": \"Classify this chest X-ray.\"},\n",
        "                            {\"type\": \"image\", \"image\": img},\n",
        "                        ],\n",
        "                    },\n",
        "                ]\n",
        "                \n",
        "                # Prepare inputs\n",
        "                inputs = processor.apply_chat_template(\n",
        "                    messages,\n",
        "                    add_generation_prompt=True,\n",
        "                    tokenize=True,\n",
        "                    return_dict=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                ).to(model.device)\n",
        "                \n",
        "                input_len = inputs[\"input_ids\"].shape[-1]\n",
        "                \n",
        "                # Generate prediction\n",
        "                with torch.inference_mode():\n",
        "                    gen = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=8,\n",
        "                        do_sample=False,\n",
        "                    )\n",
        "                \n",
        "                # Decode output\n",
        "                gen_ids = gen[0][input_len:]\n",
        "                decoded = processor.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "                texts.append(decoded)\n",
        "                \n",
        "                # Parse prediction\n",
        "                tok = decoded.split()[0].upper().strip(\".,\")\n",
        "                if \"PNEUMONIA\" in tok:\n",
        "                    preds.append(\"PNEUMONIA\")\n",
        "                    confidences.append(\"HIGH\")\n",
        "                elif \"NORMAL\" in tok:\n",
        "                    preds.append(\"NORMAL\")\n",
        "                    confidences.append(\"HIGH\")\n",
        "                else:\n",
        "                    preds.append(\"UNKNOWN\")\n",
        "                    confidences.append(\"LOW\")\n",
        "                \n",
        "                print(f\"✅ {img_id}: {preds[-1]}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {img_id}: {e}\")\n",
        "                preds.append(\"ERROR\")\n",
        "                texts.append(f\"Inference error: {str(e)[:100]}\")\n",
        "                confidences.append(\"N/A\")\n",
        "        \n",
        "        # Yield results for this batch\n",
        "        yield pd.DataFrame({\n",
        "            \"image_id\": image_ids,\n",
        "            \"file_path\": s3_paths,\n",
        "            \"prediction\": preds,\n",
        "            \"raw_text\": texts,\n",
        "            \"confidence\": confidences,\n",
        "        })\n",
        "\n",
        "print(\"✅ Inference function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-inference",
      "metadata": {},
      "source": [
        "## 7. Run Inference on Demo Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execute-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"Starting inference on demo dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run distributed inference (fetch a small sample to avoid long collect)\n",
        "result_demo_df = demo_df.mapInPandas(predict_partition_s3, schema=output_schema)\n",
        "\n",
        "# Collect a small sample instead of the full dataset\n",
        "results = result_demo_df.limit(10).collect()\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "\n",
        "print(f\"\\n✅ Inference completed in {elapsed:.2f} seconds\")\n",
        "print(f\"⏱️  Average time per image: {elapsed/len(results):.2f} seconds\\n\")\n",
        "\n",
        "# Display results\n",
        "result_demo_df.limit(10).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "full-inference",
      "metadata": {},
      "source": [
        "## 8. Run Full Dataset Inference (Production)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execute-full",
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING: This will process ALL images - can take hours!\n",
        "# Uncomment to run on full dataset\n",
        "\n",
        "# print(f\"Starting inference on {total_images:,} images...\")\n",
        "# start_time = time.time()\n",
        "\n",
        "# # Repartition for optimal parallelism\n",
        "# images_repartitioned = images_df.repartition(n_cores * 2)\n",
        "\n",
        "# # Run inference\n",
        "# results_full_df = images_repartitioned.mapInPandas(\n",
        "#     predict_partition_s3, \n",
        "#     schema=output_schema\n",
        "# )\n",
        "\n",
        "# # Cache results\n",
        "# results_full_df.cache()\n",
        "\n",
        "# end_time = time.time()\n",
        "# elapsed = end_time - start_time\n",
        "\n",
        "# print(f\"\\n✅ Full inference completed in {elapsed/60:.2f} minutes\")\n",
        "# print(f\"⏱️  Average time per image: {elapsed/total_images:.2f} seconds\")\n",
        "\n",
        "print(\"⚠️  Full dataset inference is commented out. Uncomment to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-results",
      "metadata": {},
      "source": [
        "## 9. Save Results to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-to-s3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Generate timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Define output paths\n",
        "output_parquet = f\"{RESULTS_BUCKET}/predictions_parquet/{timestamp}/\"\n",
        "output_csv = f\"{RESULTS_BUCKET}/predictions_csv/{timestamp}/\"\n",
        "\n",
        "print(f\"Saving results to S3...\")\n",
        "\n",
        "# Save as Parquet (recommended for big data)\n",
        "print(f\"\\nSaving Parquet to: {output_parquet}\")\n",
        "result_demo_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(output_parquet)\n",
        "\n",
        "print(\"✅ Parquet saved\")\n",
        "\n",
        "# Save as CSV (for human readability)\n",
        "print(f\"\\nSaving CSV to: {output_csv}\")\n",
        "result_demo_df.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(output_csv)\n",
        "\n",
        "print(\"✅ CSV saved\")\n",
        "\n",
        "# For full dataset with partitioning by prediction:\n",
        "# results_full_df.write \\\n",
        "#     .partitionBy(\"prediction\") \\\n",
        "#     .mode(\"overwrite\") \\\n",
        "#     .parquet(f\"{RESULTS_BUCKET}/predictions_partitioned/{timestamp}/\")\n",
        "\n",
        "print(f\"\\n✅ All results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analysis",
      "metadata": {},
      "source": [
        "## 10. Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analyze-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "print(\"=== Results Summary ===\")\n",
        "\n",
        "# Count by prediction\n",
        "summary = result_demo_df.groupBy(\"prediction\").count().orderBy(\"count\", ascending=False)\n",
        "print(\"\\nPrediction Distribution:\")\n",
        "summary.show()\n",
        "\n",
        "# Count errors\n",
        "error_count = result_demo_df.filter(col(\"prediction\") == \"ERROR\").count()\n",
        "print(f\"\\nTotal Errors: {error_count}\")\n",
        "\n",
        "# Show error samples if any\n",
        "if error_count > 0:\n",
        "    print(\"\\nError Samples:\")\n",
        "    result_demo_df.filter(col(\"prediction\") == \"ERROR\").select(\"image_id\", \"raw_text\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "download-results",
      "metadata": {},
      "source": [
        "## 11. Download Results from S3 (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-from-s3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download results to local machine for further analysis\n",
        "import subprocess\n",
        "\n",
        "local_download_path = \"./downloaded_results/\"\n",
        "\n",
        "print(f\"Downloading results from S3 to {local_download_path}...\")\n",
        "\n",
        "# Use AWS CLI to download\n",
        "# subprocess.run([\n",
        "#     \"aws\", \"s3\", \"sync\",\n",
        "#     output_csv.replace('s3a://', 's3://'),\n",
        "#     local_download_path\n",
        "# ])\n",
        "\n",
        "print(\"⚠️  Download command is commented out. Uncomment to execute.\")\n",
        "print(f\"\\nManual download command:\")\n",
        "print(f\"aws s3 sync {output_csv.replace('s3a://', 's3://')} {local_download_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "## 12. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "print(\"Stopping Spark session...\")\n",
        "spark.stop()\n",
        "print(\"✅ Spark session stopped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "notes",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Big Data Technologies Used:\n",
        "1. **Apache Spark** - Distributed data processing\n",
        "2. **AWS S3** - Cloud object storage (data lake)\n",
        "3. **PySpark** - Python API for Spark\n",
        "4. **Pandas UDFs (mapInPandas)** - Distributed ML inference\n",
        "5. **MedGemma** - Medical vision-language model\n",
        "6. **Parquet** - Columnar storage format\n",
        "7. **Boto3** - AWS SDK for Python\n",
        "\n",
        "### Architecture:\n",
        "```\n",
        "S3 (Raw Images) → Spark Cluster → MedGemma Inference → S3 (Results)\n",
        "       ↓                                                      ↓\n",
        "   Metadata                                         Parquet/CSV Output\n",
        "```\n",
        "\n",
        "### Performance Tips:\n",
        "- Use `.repartition()` to control parallelism\n",
        "- Cache frequently accessed DataFrames with `.cache()`\n",
        "- Use Parquet format for better compression and query performance\n",
        "- Partition output by prediction for faster queries\n",
        "- Monitor S3 costs and use lifecycle policies\n",
        "\n",
        "### Security Best Practices:\n",
        "- Use IAM roles instead of hardcoded credentials\n",
        "- Enable S3 bucket encryption\n",
        "- Use VPC endpoints for S3 access\n",
        "- Implement least privilege access policies\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "1011-hw3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
