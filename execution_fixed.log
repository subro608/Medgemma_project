Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/12/17 17:53:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[Stage 0:>                                                          (0 + 4) / 4]                                                                                [Stage 9:>                                                          (0 + 4) / 4]
[Worker-3357059] Loading Med-GEMMA on cuda (NVIDIA A100-SXM4-80GB)

[Worker-3357055] Loading Med-GEMMA on cuda (NVIDIA A100-SXM4-80GB)

[Worker-3357067] Loading Med-GEMMA on cuda (NVIDIA A100-SXM4-80GB)

[Worker-3357063] Loading Med-GEMMA on cuda (NVIDIA A100-SXM4-80GB)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.59s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.59s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.57s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.15s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.17s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.19s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.19s/it]
[Worker-3357067] ‚úÖ Model loaded in 8.62s
[Worker-3357067] Processing batch 1: 4 images
[Worker-3357059] ‚úÖ Model loaded in 8.70s
[Worker-3357059] Processing batch 1: 6 images
[Worker-3357055] ‚úÖ Model loaded in 8.70s
[Worker-3357055] Processing batch 1: 5 images
[Worker-3357063] ‚úÖ Model loaded in 8.70s
[Worker-3357063] Processing batch 1: 5 images
[Worker-3357067] ‚úÖ Batch 1 done in 27.15s
[Stage 9:==============>                                            (1 + 3) / 4][Worker-3357059] ‚úÖ Batch 1 done in 43.18s
[Stage 9:=============================>                             (2 + 2) / 4][Worker-3357063] ‚úÖ Batch 1 done in 43.73s
[Stage 9:============================================>              (3 + 1) / 4][Worker-3357055] ‚úÖ Batch 1 done in 48.88s
                                                                                25/12/17 17:54:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
25/12/17 17:54:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
================================================================================
COMPLETE END-TO-END BIG DATA PIPELINE (FIXED VERSION)
================================================================================
User: vsp7230
Scratch: /scratch/vsp7230/bigdata_project
S3 Enabled: True
Kafka Enabled: False
Test Images: 20 (balanced NORMAL/PNEUMONIA)
================================================================================


================================================================================
STEP 1: DATASET PREPARATION
================================================================================
‚úÖ Dataset already downloaded
‚úÖ Dataset organized:
   NORMAL: 1,583
   PNEUMONIA: 4,273
   TOTAL: 5,856


================================================================================
STEP 2: S3 UPLOAD (Cloud Storage Layer)
================================================================================
‚úÖ S3 bucket exists: medgemma-e2e-demo

üì§ Uploading 20 balanced images to S3...
   - NORMAL: 10
   - PNEUMONIA: 10
   Uploaded 5/20...
   Uploaded 10/20...
   Uploaded 15/20...
   Uploaded 20/20...
‚úÖ S3 upload complete: 20/20 images
‚úÖ S3 manifest created: /scratch/vsp7230/bigdata_project/outputs/s3_manifest.json

================================================================================
STEP 3: KAFKA STREAMING (Real-time Data Ingestion)
================================================================================
‚ö†Ô∏è  Kafka disabled - using direct data loading

================================================================================
STEP 4: GPU DETECTION & AUTO-TUNING
================================================================================
  GPU 0: NVIDIA A100-SXM4-80GB (79.2 GB)
‚öô  GPU Batch Size: 4
================================================================================

================================================================================
STEP 5: SPARK INITIALIZATION
================================================================================
CPU cores: 160
GPU count: 1
Spark partitions: 4
‚úÖ Spark 3.5.0 initialized

================================================================================
STEP 6: DATA LOADING (S3 ‚Üí Spark)
================================================================================
Loading data from S3 manifest...
‚úÖ Loaded 20 records from S3 manifest
‚úÖ DataFrame ready: 20 images, 4 partitions

================================================================================
STEP 7: DISTRIBUTED INFERENCE (Spark + Med-GEMMA 4B)
================================================================================
Starting distributed inference...

================================================================================
‚úÖ INFERENCE COMPLETE
================================================================================
Processed: 20 images
Time: 69.81s (1.16 min)
Throughput: 0.29 img/s
================================================================================

================================================================================
STEP 8: SAVING RESULTS (Local + S3)
================================================================================
Saving results locally...
‚úÖ Local results saved to /scratch/vsp7230/bigdata_project/outputs/results/e2e_demo_20251217_175436

üì§ Uploading results to S3...
   Uploading CSV to s3://medgemma-e2e-demo/results/e2e_demo_20251217_175436/predictions.csv...
   Uploading JSON to s3://medgemma-e2e-demo/results/e2e_demo_20251217_175436/predictions.json...
‚úÖ S3 results uploaded to s3://medgemma-e2e-demo/results/e2e_demo_20251217_175436/

================================================================================
STEP 9: ANALYTICS
================================================================================
‚úÖ latency_statistics
‚úÖ throughput_metrics
‚úÖ error_statistics

================================================================================
STEP 10: BALANCED VISUALIZATION (50% NORMAL / 50% PNEUMONIA)
================================================================================
Creating visualizations...
  ‚úÖ Dashboard saved
‚úÖ Comprehensive dashboard
  ‚úÖ GPU utilization saved
‚úÖ GPU utilization chart
  Creating X-ray + Report gallery (10 samples)...
  ‚úÖ HTML gallery saved: /scratch/vsp7230/bigdata_project/outputs/visualizations/e2e_demo_20251217_175436/xray_reports_visualization.html
‚úÖ HTML gallery (balanced): /scratch/vsp7230/bigdata_project/outputs/visualizations/e2e_demo_20251217_175436/xray_reports_visualization.html

================================================================================
STEP 11: FINAL REPORT
================================================================================

================================================================================
COMPLETE END-TO-END BIG DATA PIPELINE - EXECUTION REPORT
================================================================================

EXECUTION INFO:
  Date: 2025-12-17 17:54:41
  Run Label: e2e_demo_20251217_175436
  Duration: 1.16 minutes

TECHNOLOGY STACK:
  ‚úÖ S3 Storage: ENABLED
  ‚úÖ Kafka Streaming: DISABLED
  ‚úÖ Spark Processing: ENABLED
  ‚úÖ Med-GEMMA 4B: ENABLED
  ‚úÖ Balanced Visualization: ENABLED (5 NORMAL + 5 PNEUMONIA)

DATASET:
  Total Images: 20
  NORMAL: ~10
  PNEUMONIA: ~10

PERFORMANCE:
  Mean Latency: 8146.18 ms
  P95 Latency: 14860.45 ms
  P99 Latency: 21490.07 ms
  Throughput: 0.12 images/sec
  Daily Capacity: 10,368 images

RESULTS:
  Success: 20
  Errors: 0
  Success Rate: 100.0%

INFRASTRUCTURE:
  CPUs: 160
  GPUs: 1 (NVIDIA A100-SXM4-80GB)
  Spark Partitions: 4
  GPU Batch Size: 4

OUTPUTS:
  Local Results: /scratch/vsp7230/bigdata_project/outputs/results/e2e_demo_20251217_175436
  Local Metrics: /scratch/vsp7230/bigdata_project/outputs/metrics/e2e_demo_20251217_175436
  Local Visualizations: /scratch/vsp7230/bigdata_project/outputs/visualizations/e2e_demo_20251217_175436
  S3 Results: s3://medgemma-e2e-demo/results/e2e_demo_20251217_175436

================================================================================
‚úÖ PIPELINE EXECUTION COMPLETE
================================================================================


‚úÖ Report saved: /scratch/vsp7230/bigdata_project/outputs/results/e2e_demo_20251217_175436/execution_report.txt
‚úÖ Summary saved: /scratch/vsp7230/bigdata_project/outputs/e2e_pipeline_summary.csv

================================================================================
‚úÖ‚úÖ‚úÖ COMPLETE END-TO-END PIPELINE FINISHED ‚úÖ‚úÖ‚úÖ
================================================================================

Results: /scratch/vsp7230/bigdata_project/outputs/results/e2e_demo_20251217_175436
Visualizations: /scratch/vsp7230/bigdata_project/outputs/visualizations/e2e_demo_20251217_175436
HTML Gallery: /scratch/vsp7230/bigdata_project/outputs/visualizations/e2e_demo_20251217_175436/xray_reports_visualization.html
================================================================================
2025-12-17 17:54:42,061 - INFO - Closing down clientserver connection
